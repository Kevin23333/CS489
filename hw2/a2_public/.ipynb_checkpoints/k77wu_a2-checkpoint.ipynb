{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2 v2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q1: Logistic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\\begin{align}\n",
    "\\frac{d\\sigma(z)}{dz} &= \\frac{1}{(1 + e^{-z})^2} \\cdot \\frac{d(1 + e^{-z})}{dz}\\\\\n",
    "&= \\frac{1}{(1 + e^{-z})^2} \\cdot e^{-z} \\cdot (-1)\\\\\n",
    "&= \\frac{1}{1 + e^{-z}} \\cdot \\frac{-e^{-z}}{1 + e^{-z}}\\\\\n",
    "&= \\frac{1}{1 + e^{-z}} \\cdot (1 - \\frac{1}{1 + e^{-z}})\\\\\n",
    "&= \\sigma(z)(1 - \\sigma(z))\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To help you with $\\LaTeX$, and to show you my expectations, here is a sample taken from the lecture notes, taken from the 3rd and 4th page of the notes entitled \"Error Backpropagation\". It has nothing to do with the solution to this question, but just demonstrates some of the features of $\\LaTeX$. Notice how I include English statments to guide the reader through the derivation.\n",
    "\n",
    "<a target=_new href=\"http://detexify.kirelabs.org/classify.html\">This web page</a> is very handy for identifying $\\LaTeX$ symbols.\n",
    "\n",
    "---\n",
    "More generally, for $\\vec{x} \\in \\mathbb{R}^X$, $\\vec{h} \\in \\mathbb{R}^H$, and $\\vec{y} \\in \\mathbb{R}^Y$.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial \\alpha_i}\n",
    "  &= \\frac{d h_i}{d \\alpha_i} \\\\\n",
    "  &= \\frac{d h_i}{d \\alpha_i}\n",
    "  \\left[ M_{1i} \\ \\cdots \\ M_{Yi} \\right] \\cdot\n",
    "  \\left[ \\frac{\\partial E}{\\partial \\beta_1} \\ \\cdots \\ \\frac{\\partial E}{\\partial \\beta_Y} \\right] \\\\\n",
    "  &= \\frac{d h_i}{d \\alpha_i}\n",
    "   \\left[ M_{1i} \\ \\cdots \\ M_{Yi} \\right]\n",
    "   \\left[ \\begin{array}{c}\n",
    "     \\frac{\\partial E}{\\partial \\beta_1} \\\\\n",
    "     \\vdots \\\\\n",
    "     \\frac{\\partial E}{\\partial \\beta_Y} \\end{array} \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "Thus, for all elements,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{\\partial E}{\\partial \\alpha_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\alpha_H}\n",
    "\\end{array} \\right] &=\n",
    "%\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{d h_1}{d \\alpha_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{d h_H}{d \\alpha_H}\n",
    "\\end{array} \\right]\n",
    "\\odot\n",
    "\\left[ \\begin{array}{ccc}\n",
    "  M_{11} & \\cdots & M_{Y1} \\\\\n",
    "  \\vdots & \\ddots & \\vdots \\\\\n",
    "  M_{1H} & \\cdots & M_{YH}\n",
    "\\end{array} \\right]\n",
    "%\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{\\partial E}{\\partial \\beta_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\beta_Y}\n",
    "\\end{array} \\right] \\\\\n",
    "%\n",
    "\\frac{\\partial E}{\\partial \\vec{\\alpha}} &=\n",
    "\\frac{d \\vec{h}}{d \\vec{\\alpha}} \\odot M^\\mathrm{T}\n",
    "\\frac{\\partial E}{\\partial \\vec{\\beta}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q2: Multiclass Contrast Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The likelihood of $(\\vec{x}^{(i)}, \\vec{t}^{(i)})$ is\n",
    "\\begin{align}\n",
    "l(\\theta) &= \\prod_{k = 1}^{K}Pr\\left(\\vec{x}^{(i)} \\in C_k\\big | \\theta\\right)\\\\\n",
    "&= \\prod_{k = 1}^{K}\\left(y_k^{(i)}\\right)^{t_k^{(i)}}\\left(1 - y_k^{(i)}\\right)^{\\left(1 - t_k^{(i)}\\right)}\n",
    "\\end{align}\n",
    "\n",
    "Take log on both sides, then we gets the log-likelihood\n",
    "\\begin{equation}\n",
    "ll(\\theta) = \\sum_{k = 1}^{K}\\left[t_k^{(i)}lny_k^{(i)} + \\left(1 - t_k^{(i)}\\right)ln\\left(1 - y_k^{(i)}\\right)\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Thus, the negative log-likelihood is\n",
    "\\begin{equation}\n",
    "-\\sum_{k = 1}^{K}\\left[t_k^{(i)}lny_k^{(i)} + \\left(1 - t_k^{(i)}\\right)ln\\left(1 - y_k^{(i)}\\right)\\right]\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q3: Top-Layer Error Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z} &= \\frac{\\partial}{\\partial z}\\left[- \\sum_{k = 1}^{Y} t_klny_k + (1 - t_k)ln(1 - y_k)\\right]\\\\\n",
    "&= -\\sum_{k = 1}^{Y}\\left[t_k\\frac{\\partial}{\\partial z}lny_k + (1 - t_k)\\frac{\\partial}{\\partial z}(1 - y_k)\\right]\\\\\n",
    "&= -\\sum_{k = 1}^{Y}\\left[\\frac{t_k}{y_k}\\frac{\\partial y_k}{\\partial z} - \\frac{1 - t_k}{1 - y_k}\\frac{\\partial y_k}{\\partial z}\\right]\\\\\n",
    "&= \\sum_{k = 1}^{Y}\\left[\\frac{y_k - t_k}{y_k(1 - y_k)}\\frac{\\partial y_k}{\\partial z}\\right]\\\\\n",
    "&= \\sum_{k = 1}^{Y}\\frac{y_k - t_k}{y_k(1 - y_k)}\\left[\\begin{matrix}\\vdots\\\\ \\sigma(z_k)(1 - \\sigma(z_k))\\\\ \\vdots\\end{matrix}\\right]\\\\\n",
    "&= \\sum_{k = 1}^{Y}\\left[\\begin{matrix}\\vdots\\\\  y_k - t_k\\\\ \\vdots\\end{matrix}\\right]\\\\\n",
    "&= y - t\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z} &= \\frac{\\partial}{\\partial z}\\sum_{k = 1}^{Y} (y_k - t_k)^2\\\\\n",
    "&= 2\\sum_{k = 1}^{Y}(y_k - t_k) \\frac{\\partial y_k}{\\partial z}\\\\\n",
    "&= 2\\sum_{k = 1}^{Y}(y_k - t_k)\\left[\\begin{matrix}\\vdots\\\\ 1\\\\ \\vdots\\end{matrix}\\right]\\\\\n",
    "&= 2\\sum_{k = 1}^{Y}\\left[\\begin{matrix}\\vdots\\\\ (y_k - t_k)\\\\ \\vdots\\end{matrix}\\right]\\\\\n",
    "&= 2(y - t)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Q4: Implementing Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Supplied functions\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from ipywidgets import FloatProgress  \n",
    "import time\n",
    "\n",
    "def Logistic(z):\n",
    "    return 1. / (1 + np.exp(-z) )\n",
    "\n",
    "def Identity(z):\n",
    "    return z\n",
    "\n",
    "def OneHot(z):\n",
    "    y = np.zeros(np.shape(z))\n",
    "    y[np.argmax(z)] = 1.\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "code_folding": [
     2,
     51,
     68,
     69,
     86,
     143,
     151
    ],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "\n",
    "    def __init__(self, sizes, type='classifier'):\n",
    "        '''\n",
    "            net = Network.Network(sizes, type='classifier')\n",
    "\n",
    "            Creates a Network and saves it in the variable 'net'.\n",
    "\n",
    "            Inputs\n",
    "              sizes is a list of integers specifying the number\n",
    "                  of nodes in each layer\n",
    "                  eg. [5, 20, 3] will create a 3-layer network\n",
    "                      with 5 input, 20 hidden, and 3 output nodes\n",
    "              type can be either 'classifier' or 'regression', and\n",
    "                  sets the activation function on the output layer,\n",
    "                  as well as the loss function.\n",
    "                  'classifier': logistic, cross entropy\n",
    "                  'regression': linear, mean squared error\n",
    "        '''\n",
    "        self.n_layers = len(sizes)\n",
    "        self.N = sizes   # array of number of nodes per layer\n",
    "        self.h = []      # node activities, one array per layer\n",
    "        self.z = []      # input current, one array per layer\n",
    "        self.b = []      # biases, one array per layer\n",
    "        self.W = []      # Weight matrices, indexed by the layer below it\n",
    "        self.dEdb = []   # Gradient of loss w.r.t. biases\n",
    "        self.dEdW = []   # Gradient of loss w.r.t. weights\n",
    "        for n in self.N:\n",
    "            self.h.append(np.zeros(n))\n",
    "            self.z.append(np.zeros(n))\n",
    "            self.dEdb.append(np.zeros(n))\n",
    "            self.b.append(np.random.normal(size=n)/np.sqrt(n))\n",
    "        for k in range(len(self.N)-1):\n",
    "            self.W.append(np.random.normal(size=[self.N[k+1], self.N[k]])/np.sqrt(self.N[k]))\n",
    "            self.dEdW.append(np.zeros([self.N[k+1], self.N[k]]))\n",
    "\n",
    "        # Two common types of networks\n",
    "        # The member variable self.Loss refers to one of the implemented\n",
    "        # loss functions: MSE, or CrossEntropy.\n",
    "        # Call it using self.Loss(t)\n",
    "        self.Activation = Logistic\n",
    "        if type=='classifier':\n",
    "            self.classifier = True\n",
    "            self.Loss = self.CrossEntropy\n",
    "            self.OutputActivation = Logistic\n",
    "        else:\n",
    "            self.classifier = False\n",
    "            self.Loss = self.MSE\n",
    "            self.OutputActivation = Identity\n",
    "\n",
    "    def FeedForward(self, x):\n",
    "        '''\n",
    "            y = net.FeedForward(x)\n",
    "\n",
    "            Runs the network forward, starting with x as input.\n",
    "            Returns the activity of the output layer.\n",
    "\n",
    "            All node use \n",
    "            Note: The activation function used for the output layer\n",
    "            depends on what self.Loss is set to.\n",
    "        '''\n",
    "        self.h[0] = x # set input layer\n",
    "\n",
    "        # == YOUR CODE HERE ==\n",
    "        \n",
    "        \n",
    "        for k in range(1, self.n_layers - 1):\n",
    "            self.z[k] = np.dot(self.W[k - 1], self.h[k - 1]) + self.b[k]\n",
    "            self.h[k] = Logistic(self.z[k])\n",
    "        \n",
    "        self.z[self.n_layers - 1] = np.dot(self.W[self.n_layers - 2], self.h[self.n_layers - 2]) + self.b[self.n_layers - 1]\n",
    "        self.h[self.n_layers - 1] = self.OutputActivation(self.z[self.n_layers - 1])\n",
    "        \n",
    "            \n",
    "        # Return activity of output layer\n",
    "        return self.h[self.n_layers - 1]\n",
    "\n",
    "    def Evaluate(self, data):\n",
    "        '''\n",
    "            E = net.Evaluate(data)\n",
    "\n",
    "            Computes the average loss over the supplied dataset.\n",
    "\n",
    "            Inputs\n",
    "              data is a list of 2 arrays containing inputs and targets\n",
    "\n",
    "            Outputs\n",
    "              E is a scalar, the average loss\n",
    "        '''\n",
    "        total_loss = 0.\n",
    "        for x, t in zip(data[0], data[1]):\n",
    "            self.FeedForward(x)\n",
    "            total_loss += self.Loss(t)\n",
    "        return total_loss / len(data[0])\n",
    "\n",
    "    def ClassificationAccuracy(self, data):\n",
    "        '''\n",
    "            a = net.ClassificationAccuracy(data)\n",
    "            \n",
    "            Returns the fraction (between 0 and 1) of correct one-hot classifications\n",
    "            in the dataset.\n",
    "        '''\n",
    "        n_correct = 0\n",
    "        for x, t in zip(data[0], data[1]):\n",
    "            y = self.FeedForward(x)\n",
    "            yb = OneHot(y)\n",
    "            if np.argmax(yb)==np.argmax(t):\n",
    "                n_correct += 1\n",
    "        return float(n_correct) / len(data[0])\n",
    "\n",
    "    def CrossEntropy(self, t):\n",
    "        '''\n",
    "            E = net.CrossEntropy(t)\n",
    "\n",
    "            Evaluates the cross entropy loss function using t and the activity of the top layer.\n",
    "            To evaluate the network's performance on an input/output pair (x,t), use\n",
    "              net.FeedForward(x)\n",
    "              E = net.Loss(t)\n",
    "\n",
    "            Inputs\n",
    "              t is an array holding the target output\n",
    "\n",
    "            Outputs\n",
    "              E is the loss function for the given case\n",
    "        '''\n",
    "        E = 0.  # placeholder code\n",
    "        \n",
    "        # == YOUR CODE HERE ==\n",
    "        \n",
    "        for k in range(len(t)):\n",
    "            E += t[k] * np.log(self.h[self.n_layers - 1][k]) + \\\n",
    "                        (1 - t[k]) * np.log(1 - self.h[self.n_layers - 1][k])\n",
    "        \n",
    "        E = -E\n",
    "            \n",
    "        return E\n",
    "\n",
    "    def MSE(self, t):\n",
    "        '''\n",
    "            E = net.MSE(t)\n",
    "\n",
    "            Evaluates the MSE loss function using t and the activity of the top layer.\n",
    "            To evaluate the network's performance on an input/output pair (x,t), use\n",
    "              net.FeedForward(x)\n",
    "              E = net.Loss(t)\n",
    "\n",
    "            Inputs\n",
    "              t is an array holding the target output\n",
    "\n",
    "            Outputs\n",
    "              E is the loss function for the given case\n",
    "        '''\n",
    "        E = 0.  # placeholder code\n",
    "        \n",
    "        # == YOUR CODE HERE ==\n",
    "        for k in range(len(t)):\n",
    "            E += np.power(t[k] - self.h[self.n_layers - 1][k], 2)\n",
    "            \n",
    "        return E\n",
    "\n",
    "    def BackProp(self, t, lrate=0.05):\n",
    "        '''\n",
    "            Given a target t, updates the connection weights and biases using the\n",
    "            backpropagation algorithm.\n",
    "        '''\n",
    "        \n",
    "        # == YOUR CODE HERE ==\n",
    "        \n",
    "        #calculate the gradient\n",
    "        if self.classifier == True:\n",
    "            dEdz = self.h[self.n_layers - 1] - t\n",
    "        else:\n",
    "            dEdz = 2 * (self.h[self.n_layers - 1] - t)\n",
    "            \n",
    "                     \n",
    "        for k in range(self.n_layers - 1, 0, -1):\n",
    "            self.dEdW[k - 1] = np.outer(dEdz, self.h[k - 1])\n",
    "            self.dEdb[k] = dEdz\n",
    "            dEdz = np.dot(self.W[k - 1].T, dEdz) * self.h[k - 1] * (1 - self.h[k - 1])\n",
    "\n",
    "        #update weights\n",
    "        for k in range(self.n_layers - 1, 0, -1):\n",
    "            self.W[k - 1] += - lrate * self.dEdW[k - 1]\n",
    "            self.b[k] += - lrate * self.dEdb[k]\n",
    "\n",
    "    def Learn(self, data, lrate=0.05, epochs=1, progress=True):\n",
    "        '''\n",
    "            Network.Learn(data, lrate=0.05, epochs=1, progress=True)\n",
    "\n",
    "            Run through the dataset 'epochs' number of times, incrementing the\n",
    "            network weights for each training sample. For each epoch, it\n",
    "            shuffles the order of the samples.\n",
    "\n",
    "            Inputs\n",
    "              data is a list of 2 arrays, one for inputs, and one for targets\n",
    "              lrate is the learning rate (try 0.001 to 0.5)\n",
    "              epochs is the number of times to go through the training data\n",
    "              progress (Boolean) indicates whether to show a progress bar\n",
    "        '''\n",
    "        # Here is some useful code for displaying a progress bar\n",
    "        data_shuffled = list(zip(data[0],data[1]))\n",
    "        if progress:\n",
    "            fp = FloatProgress(min=0,max=100)  \n",
    "            display(fp)\n",
    "        \n",
    "        # == YOUR CODE HERE ==\n",
    "        \n",
    "        \n",
    "        for ep in range(epochs):\n",
    "            np.random.shuffle(data_shuffled)\n",
    "            for x, t in data_shuffled:\n",
    "                self.FeedForward(x)\n",
    "                self.BackProp(t, lrate)\n",
    "            \n",
    "            #if progress:\n",
    "            #  print('epochs: %d, loss: %f' % (ep, self.Evaluate(data)))\n",
    "                \n",
    "            if progress:\n",
    "                fp.value += 1 / epochs * 100\n",
    "        \n",
    "        # For the progress bar, simply increment fp.value by whatever you want.\n",
    "        # The progress bar will be full when fp.value = the max given in the\n",
    "        # FloatProgress call above.\n",
    "        \n",
    "        #fp.value += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Create a Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 5 Classes in 8-Dimensional Space\n",
    "InputClasses = np.array([[1,0,1,0,0,1,1,0],\n",
    "                         [0,1,0,1,0,1,0,1],\n",
    "                         [0,1,1,0,1,0,0,1],\n",
    "                         [1,0,0,0,1,0,1,1],\n",
    "                         [1,0,0,1,0,1,0,1]], dtype=float)\n",
    "OutputClasses = np.array([[1,0,0,0,0],\n",
    "                          [0,1,0,0,0],\n",
    "                          [0,0,1,0,0],\n",
    "                          [0,0,0,1,0],\n",
    "                          [0,0,0,0,1]], dtype=float)\n",
    "n_input = np.shape(InputClasses)[1]\n",
    "n_output = np.shape(OutputClasses)[1]\n",
    "n_classes = np.shape(InputClasses)[0]\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 100\n",
    "training_output = []\n",
    "training_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)\n",
    "    t = OutputClasses[k,:]\n",
    "    training_input.append(x)\n",
    "    training_output.append(t)\n",
    "\n",
    "# Create a test dataset\n",
    "n_samples = 100\n",
    "test_output = []\n",
    "test_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)\n",
    "    t = OutputClasses[k,:]\n",
    "    test_input.append(x)\n",
    "    test_output.append(t)\n",
    "\n",
    "train = [training_input, training_output]\n",
    "test = [test_input, test_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a Network\n",
    "net = Network([n_input, 10, n_output], type='classifier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy = 3.41161463505\n",
      "     Accuracy = 17.0%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate it before training\n",
    "CE = net.Evaluate(train)\n",
    "accuracy = net.ClassificationAccuracy(train)\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2b9af7f93b429db4ec204f437fe1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.Learn([training_input, training_output], epochs=500, progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy = 0.120857034292\n",
      "     Accuracy = 100.0%\n"
     ]
    }
   ],
   "source": [
    "# On training dataset\n",
    "CE = net.Evaluate(train)\n",
    "accuracy = net.ClassificationAccuracy(train)\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy = 5.4708498192\n",
      "     Accuracy = 48.0%\n"
     ]
    }
   ],
   "source": [
    "# On test dataset\n",
    "CE = net.Evaluate(test)\n",
    "accuracy = net.ClassificationAccuracy(test)\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create a Regression Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1D -> 1D (linear mapping)\n",
    "n_input = 1\n",
    "n_output = 1\n",
    "slope = np.random.rand() - 0.5\n",
    "intercept = np.random.rand()*2. - 1.\n",
    "\n",
    "def myfunc(x):\n",
    "    return slope*x+intercept\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 100\n",
    "training_output = []\n",
    "training_input = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx]\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    training_input.append(np.array([x]))\n",
    "    training_output.append(np.array([t]))\n",
    "\n",
    "# Create a testing dataset\n",
    "n_samples = 100\n",
    "test_input = []\n",
    "test_output = []\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx] + np.random.normal(scale=0.1)\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    test_input.append(np.array([x]))\n",
    "    test_output.append(np.array([t]))\n",
    "\n",
    "# Create a perfect dataset\n",
    "n_samples = 100\n",
    "perfect_input = []\n",
    "perfect_output = []\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = test_input[idx]\n",
    "    t = myfunc(x[0])\n",
    "    perfect_input.append(np.array(x))\n",
    "    perfect_output.append(np.array([t]))\n",
    "    \n",
    "train = [training_input, training_output]\n",
    "test = [test_input, test_output]\n",
    "perfect = [perfect_input, perfect_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = Network([1, 20, 1], type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 3.95749666612\n"
     ]
    }
   ],
   "source": [
    "# Evaluate it before training\n",
    "mse = net.Evaluate(train)\n",
    "print('MSE = '+str(mse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e330227eb37e4728b22bbc6cf70eeb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.Learn(train, epochs = 500, progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE = 0.00877468331355\n"
     ]
    }
   ],
   "source": [
    "# On training dataset\n",
    "mse = net.Evaluate(train)\n",
    "print('Training MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE = 0.0121626483551\n"
     ]
    }
   ],
   "source": [
    "# On test dataset\n",
    "mse = net.Evaluate(test)\n",
    "print('Test MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfect MSE = 0.000910037501403\n"
     ]
    }
   ],
   "source": [
    "# On perfect dataset\n",
    "mse = net.Evaluate(perfect)\n",
    "print('Perfect MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Evaluate our model and the TRUE solution (since we know it)\n",
    "s = np.linspace(-1, 1, 200)\n",
    "y = [net.FeedForward([x]) for x in s]\n",
    "p = [myfunc(x) for x in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e1600715f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training data,\n",
    "# as well as out model and the true model\n",
    "plt.plot(s,y, 'r--')\n",
    "plt.plot(s,p, 'g:')\n",
    "plt.plot(training_input, training_output, 'bo')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  6.08943846e-03],\n",
      "       [  6.20142779e-05],\n",
      "       [ -7.36426324e-03],\n",
      "       [ -4.02745722e-03],\n",
      "       [  8.73478334e-03],\n",
      "       [  1.43536147e-02],\n",
      "       [  1.68182389e-02],\n",
      "       [  1.04919634e-02],\n",
      "       [ -3.43742592e-03],\n",
      "       [ -5.10204853e-03],\n",
      "       [ -1.64346705e-03],\n",
      "       [  3.01489613e-03],\n",
      "       [ -6.57000938e-03],\n",
      "       [  1.22322828e-02],\n",
      "       [ -1.01497478e-02],\n",
      "       [  1.58817204e-02],\n",
      "       [  2.46725822e-02],\n",
      "       [ -8.18125968e-03],\n",
      "       [ -6.63732348e-03],\n",
      "       [  9.42836839e-03]]), array([[-0.15537101, -0.06843523, -0.09471943, -0.17078743, -0.08573887,\n",
      "        -0.17964999, -0.07824375, -0.11183053, -0.21443417, -0.05635493,\n",
      "        -0.09883636, -0.09691166, -0.24654459, -0.09644854, -0.13614749,\n",
      "        -0.2151946 , -0.0683527 , -0.0302481 , -0.11807671, -0.09033815]])]\n",
      "[-0.29147246]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
